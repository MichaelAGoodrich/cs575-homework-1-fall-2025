{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice with GNNs\n",
    "\n",
    "The purpose of this homework is for you to build intuition about graph neural networks. You will explore three different neural network learning approaches\n",
    "\n",
    "- A graph autoencoder that uses one-hot encodings for the node feature vector\n",
    "- A graph convolutional neural network that uses a node feature vector obtained using a transformer\n",
    "- A graph convolution neural network that includes some training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and Utilities\n",
    "\n",
    "**Imports:** Import all libraries that will be used in this notebook. Sort them into categories so that it's easy to see how they might be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ebc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN libraries\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GAE\n",
    "\n",
    "# Numpy and random\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Matplot lib utilities\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.axes import Axes\n",
    "\n",
    "\n",
    "# Network x\n",
    "import networkx as nx\n",
    "\n",
    "# Useful utilities for understanding results\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib.patches import Patch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Data  and type management\n",
    "from numpy.typing import NDArray\n",
    "from typing import Hashable\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drawing Utilities:** Functions that we'll use to show results. We'll look at networks with labeled nodes so we'll need utilities for plotting networks with labeled nodes and with colored nodes. It will also be helpful to inspect clusters of the embeddings using a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(embeddings, labels, title=\"t-SNE of Node Embeddings\", cmap=plt.cm.tab10):\n",
    "    \"\"\"Plot a 2D t-SNE scatterplot of node embeddings colored by labels.\"\"\"\n",
    "    z = TSNE(n_components=2, perplexity=5, random_state=42).fit_transform(embeddings)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    _ = plt.scatter(z[:, 0], z[:, 1], c=labels, cmap=cmap, s=60, edgecolors='k')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def plot_ground_truth_graph(G, node_labels, title=\"Graph with Node Labels\", cmap=plt.cm.tab10):\n",
    "    \"\"\"Visualize a NetworkX graph with nodes colored by labels.\"\"\"\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    nx.draw(G, pos, node_color=node_labels, cmap=cmap, node_size=100, with_labels=False)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    legend_handles = [Patch(color=plt.cm.tab10(i), label=label) for i, label in enumerate(unique_labels)]\n",
    "    plt.legend(handles=legend_handles, title=\"Subjects\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "def plot_graph(G: nx.Graph, \n",
    "               node_labels: list[int],\n",
    "               pos: dict[Hashable, tuple[float, float]] | None = None,\n",
    "               title: str = \" \", \n",
    "               ax: Axes | None = None) -> None:\n",
    "    if pos is None:\n",
    "        pos = nx.spring_layout(G, seed=42)\n",
    "    if ax is None:\n",
    "        plt.figure()\n",
    "        ax = plt.gca()\n",
    "    nx.draw(G, \n",
    "            pos = None, \n",
    "            node_color=node_labels, \n",
    "            cmap=plt.cm.tab10,\n",
    "            node_size=100, \n",
    "            ax = ax, \n",
    "            with_labels=False)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dataset\n",
    "\n",
    "We'll construct a smaller dataset from the [Cora dataset](https://paperswithcode.com/dataset/cora). ChatGPT-4o helped with code to subsample this dataset to something more managable.\n",
    "\n",
    "**Import:** Import information about the nodes. The dataset assigns a unique paper ID to each paper. The dataset also includes a bag of words feature vector with 1433 words. Label the columns in the dataset with \"feature_num\". Label the last column with the class of the paper, which in the Cora dataset is the subject area into which the paper was categorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load node data\n",
    "column_names = ['paper_id'] + [f'feature_{i}' for i in range(1433)] + ['subject']\n",
    "nodes: pd.DataFrame  = pd.read_csv('datasets/cora.content', sep='\\t', header=None, names=column_names)\n",
    "nodes.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the edges in the dataset, which is given by which papers cite which other papers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load edge data\n",
    "edges: pd.DataFrame = pd.read_csv('datasets/cora.cites', sep='\\t', header=None, names=['source', 'target'])\n",
    "edges.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Networkx Graph\n",
    "\n",
    "**Create Graph from DataFrame:** Create an undirected networkx graph. Nodes are indexed by the paper ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph\n",
    "G: nx.Graph = nx.from_pandas_edgelist(edges, 'source', 'target', create_using=nx.Graph())\n",
    "\n",
    "# Add node attributes\n",
    "for _, row in nodes.iterrows():\n",
    "    G.nodes[row['paper_id']].update(row.to_dict())\n",
    "\n",
    "# Show first 5 entries\n",
    "num_nodes: int = 5\n",
    "for node in G.nodes():\n",
    "    print(f\"node {node} is in class {G.nodes[node]['subject']} = {G.nodes[node]['feature_0']}\")\n",
    "    num_nodes -=1\n",
    "    if num_nodes == 0:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a connected subgraph:** The full Cora database makes it a bit difficult to see results, so create a connected subgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from ChatGPT in response to prompt of how to create smaller dataset\n",
    "# with from Cora while ensuring that the resulting graph is connected\n",
    "def snowball_sample(G_full: nx.Graph, start_node: str, target_size: int) -> nx.Graph:\n",
    "    visited = set([start_node])\n",
    "    frontier = set(G_full.neighbors(start_node))\n",
    "    \n",
    "    while len(visited) < target_size and frontier:\n",
    "        next_node = random.choice(list(frontier))\n",
    "        visited.add(next_node)\n",
    "        frontier.update(G_full.neighbors(next_node))\n",
    "        frontier -= visited  # remove already visited nodes\n",
    "    \n",
    "    return G_full.subgraph(visited).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure consistent results for everyone doing the assignment\n",
    "random.seed(42)\n",
    "\n",
    "# Start from a high-degree node\n",
    "start_node = max(G.degree, key=lambda x: x[1])[0]\n",
    "\n",
    "# Sample 250 connected nodes\n",
    "G_sub = snowball_sample(G, start_node=start_node, target_size=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map class labels to integers\n",
    "labels = nx.get_node_attributes(G_sub, 'subject')\n",
    "unique_labels = sorted(set(labels.values()))\n",
    "label_to_int = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "# Create a color list for the nodes\n",
    "node_colors = [label_to_int[labels[node]] for node in G_sub.nodes()]\n",
    "\n",
    "# Show\n",
    "plot_ground_truth_graph(G_sub, node_colors, title=\"Cora Subgraph (250 Nodes) Colored by Class Label\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how some classes don't have as many samples as others. Indeed, there are no nodes from the _Probabilistic Methods_ class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count: dict[str, int] = {subject: 0 for subject in set(nodes['subject'])}\n",
    "for node in G_sub.nodes():\n",
    "    class_count[G_sub.nodes[node]['subject']] += 1\n",
    "print(class_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build PyTorch Data Structures\n",
    "\n",
    "We will explore three types of models:\n",
    "- Graph autoencoders that use one-hot encoding for each node to create a node embedding\n",
    "- Graph convolutional neural networks that use the node's feature vector to create a node embedding\n",
    "- Graph convolutional neural networks that use the node's feature vector and some hand-labeled nodes to do semi-supervised learning.\n",
    "\n",
    "Each model will be built using a class in the PyTorch Geometric library. The input to these models will be the data from the graph, and this data must be put into the format required by PyTorch. The dataformat is abbreviated _PyG_ for PyTorch Geometric. It needs to have edge information (since we are doing graph neural networks), node features, node labels, and information about training data (if using semi-supervised learning).\n",
    "\n",
    "I used ChatGPT-4o to put the data into the correct format. Two formats are sufficient: one with one-hot encoding and one with node features plus training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot Encoding**\n",
    "\n",
    "Build PyG data structure for data using one-hot encoding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# Step 1: One-hot encode the 250 nodes in the subgraph\n",
    "ordered_nodes = list(G_sub.nodes())\n",
    "num_nodes = len(ordered_nodes)\n",
    "x = torch.eye(num_nodes)  # Shape: (250, 250), each row is a one-hot vector\n",
    "\n",
    "# Step 2: Build node ID → index mapping\n",
    "node_to_index = {node_id: i for i, node_id in enumerate(ordered_nodes)}\n",
    "\n",
    "# Step 3: Remap edges using index mapping\n",
    "edges = [\n",
    "    [node_to_index[src], node_to_index[dst]]\n",
    "    for src, dst in G_sub.edges()\n",
    "]\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).T  # Shape: (2, num_edges)\n",
    "\n",
    "# Step 4: Map class labels to integers\n",
    "labels = nx.get_node_attributes(G_sub, 'subject')\n",
    "unique_labels = sorted(set(labels.values()))\n",
    "label_to_index = {label: i for i, label in enumerate(unique_labels)}\n",
    "y = torch.tensor([label_to_index[labels[node]] for node in ordered_nodes], dtype=torch.long)\n",
    "\n",
    "# Step 5: Create PyG Data object\n",
    "data_onehot: Data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Inspect\n",
    "print(f\"x shape: {data_onehot.x.shape}, edge_index shape: {data_onehot.edge_index.shape}, y shape: {data_onehot.y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Node Feature Vector**\n",
    "\n",
    "Encode the feature of the input using the bag-of-words labels in the Cora dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare node features and labels\n",
    "X = nodes.iloc[:, 1:-1].values  # 1433 BoW features\n",
    "y = nodes[\"subject\"]\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Map paper_id to index\n",
    "paper_id_to_idx = {paper_id: i for i, paper_id in enumerate(nodes[\"paper_id\"])}\n",
    "\n",
    "# Build edge index for subgraph\n",
    "ordered_nodes = list(G_sub.nodes())\n",
    "node_to_index = {node_id: i for i, node_id in enumerate(ordered_nodes)}\n",
    "edge_list = [\n",
    "    [node_to_index[src], node_to_index[dst]]\n",
    "    for src, dst in G_sub.edges()\n",
    "    if src in node_to_index and dst in node_to_index\n",
    "]\n",
    "edge_index = torch.tensor(edge_list, dtype=torch.long).T\n",
    "\n",
    "# Build node feature matrix and label vector (aligned with ordered_nodes)\n",
    "X_sub = torch.tensor([X[paper_id_to_idx[n]] for n in ordered_nodes], dtype=torch.float)\n",
    "y_sub = torch.tensor([y_encoded[paper_id_to_idx[n]] for n in ordered_nodes], dtype=torch.long)\n",
    "\n",
    "data_cora: Data = Data(x=X_sub, edge_index=edge_index, y=y_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Node Labels**\n",
    "\n",
    "We'll eventually use the node labels, so let's randomly choose some of the nodes in the graph and use the known classes for those nodes as labeled training data. We'll add this to the existing data structure for simplicity. We'll only use 10% of the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Choose what percentage of data to use as training data\n",
    "training_percent = 0.1\n",
    "\n",
    "# Step 1: Build training mask with ~10% nodes (balanced across classes)\n",
    "num_classes = len(set(y_sub.tolist())) + 1\n",
    "num_total = len(y_sub)\n",
    "num_train = int(training_percent * num_total)\n",
    "\n",
    "# Group indices by class\n",
    "class_indices = defaultdict(list)\n",
    "for i, label in enumerate(y_sub.tolist()):\n",
    "    class_indices[label].append(i)\n",
    "\n",
    "# Sample evenly from each class\n",
    "samples_per_class = num_train // num_classes\n",
    "train_indices = []\n",
    "\n",
    "for label, indices in class_indices.items():\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices.extend(indices[:samples_per_class])\n",
    "\n",
    "# Build training mask\n",
    "train_mask = torch.zeros(num_total, dtype=torch.bool)\n",
    "train_mask[train_indices] = True\n",
    "data_cora.train_mask = train_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GNN Architecture\n",
    "\n",
    "To make comparisons fair, we'll use essentially the same architecture for each GNN we build. The figure below was generated by ChatGPT-4o with prompts by me. It has obvious errors but illustrates the basic idea.\n",
    "\n",
    "<img src=\"figures/Graph_Convolutional_Network_Architecture.png\" alt=\"General architecture for each GNN\" width = \"800\">\n",
    "\n",
    "The architecture has two hidden layers, and the nonlinear \"squashing\" function between each layer will be an ReLU. Each GNN will use the same dimension for the hidden layers and for the output (except for the semi-supervised implementation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_1_NODES=128\n",
    "HIDDEN_2_NODES=64 \n",
    "EMBEDDING_DIMENSION=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've made no effort to optimize the number of layers or the number of nodes per layer. I've also made no effort to optimize the nonlinear squashing function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Autoencoder\n",
    "\n",
    "**Architecture:** Define a GAE model with two hidden layers and relu's between the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden1, hidden2, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden1)\n",
    "        self.conv2 = GCNConv(hidden1, hidden2)\n",
    "        self.conv3 = GCNConv(hidden2, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss the elements of this class not only because it is a nice review but also because each of the GNNs that we'll define will use this identical architecture.\n",
    "\n",
    "The `__init__` function inherits from the `torch.nn.Module` superclass and instantiates three convolutional layers: one for the first hidden layer, one for the second hidden layer, and one for the output layer. \n",
    "\n",
    "The `foward` function strings together the outputs from the previous layer with the inputs of the next layer. Stated simply, it passes the input \"forward\" through each layer, squashing things from one layer to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training:** Add the training function, which just sequences the steps required for training. They key part of the training function is the definition of loss: `loss = model.recon_loss(z, data.edge_index)`. This says to use the reconstruction loss. In case you've forgotten, this just means that the goal of this network is to take the embedding and turn it into the the adjacency matrix using the following steps.\n",
    "\n",
    "Recall that the goal is to take node $i$, call it $u_i$, and compute a real-valued vector representation of the node, call it ${\\mathbf z}_i$. We want the embedding to satisfy the property that two similar nodes end up close to each other in the embedding space.\n",
    "\n",
    "- if ${\\rm sim}(u_i, u_j)$ is high then ${\\mathbf z}_i$ is near ${\\mathbf z}_j$.\n",
    "\n",
    "For a graph convolutional autoencoder, we need to define what we mean both by _similar_ and by _near_. \n",
    "\n",
    "- We'll use adjacency to define _similar_, so two nodes are similar if $A_{ij}=1$.\n",
    "- We'll use cosine similarity as the metric for _near_, so we want ${\\mathbf z}_i^T{\\mathbf z}_j$ to be high.\n",
    "\n",
    "The maximum value of the cosine between two vectors is 1, and the minimum value for the cosine between two vectors is -1. And since we aren't dividing by the length of ${\\mathbf z}_i$ and ${\\mathbf z}_j$ like we technicall have to do if we want the vector product to represent actual cosine, we aren't guaranteed that ${\\mathbf z}_i^T{\\mathbf z}_j$ will even be between $-1$ and $1$. To fix this, we'll pass this product through the sigmoid function, which means that _near_ is defined as\n",
    "\n",
    "$$ \\sigma({\\mathbf z}_i^T{\\mathbf z}_j) = \\frac{1}{1+e^{-{\\mathbf z}_i^T{\\mathbf z}_j}}$$\n",
    "\n",
    "which squashes the values of _near_ so that they are always between $0$ and $1$. In other words, we'll approximate $A_{ij}$ by $\\sigma({\\mathbf z}_i^T{\\mathbf z}_j)$.  The decoder does this computation for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    loss = model.recon_loss(z, data.edge_index)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the model and train. \n",
    "- The encoder instantiates the GCN architecture using the required number of input and hidden layers. Its goal is to output the embedding.\n",
    "- The `model` is set to `GAE(encoder)`. This is the GAE decoder, which tries to produce the adjacency matrix using the math described above\n",
    "- The `model.recon_loss` uses a specific loss function designed to efficiently compute error between the actual adjacency matrix and the predicted adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_epochs: int = 200\n",
    "\n",
    "# Model setup\n",
    "encoder = GCNEncoder(in_channels=data_onehot.num_node_features, hidden1=HIDDEN_1_NODES, hidden2=HIDDEN_2_NODES, out_channels=EMBEDDING_DIMENSION)\n",
    "model = GAE(encoder)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss = train(model, data_onehot, optimizer)\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:3d} | Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding dimension is pretty big (32 dimensions) so we can't visualize it very well. We can get a sense for whether or not there is good clustering by projecting the 32 dimensions down to 2 dimensions using the TSNE algorithm, which tries to choose the values in the 2 dimensions so that the result is easy to visualize. The plot therefore gives us a sense of whether the embedding can be used to identify similar nodes by how well they cluster in the embedding space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the mode in evaluation (instead of training) mode\n",
    "model.eval()\n",
    "\n",
    "# Tell PyTorch to not track gradients and then infer the embedding\n",
    "with torch.no_grad():\n",
    "    node_embeddings_GAE = model.encode(data_onehot.x, data_onehot.edge_index)  # shape: [num_nodes, embedding_dim]\n",
    "\n",
    "plot_scatter(node_embeddings_GAE, data_onehot.y.cpu(), title=\"t-SNE 2D Projection of GAE Node Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Answer this question before running the next cell. \n",
    "\n",
    "Based on patterns you see in the scatterplot above, what will happen when the 32-dimensional embeddings are clustered into 6 clusters (since the seventh class never appears in the data)? In other words, suppose that we find six clusters and then label each node by its cluster. How well will these node labels correspond to the actual node labels? Justify your answer by looking at the ground truth plot we generated above and discussing what kinds of correlations you see between the graph structure and the node class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1\n",
    "\n",
    "Put your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Run the following cell, which shows a side-by-side comparison of the node classes of ground truth versus the labels produced by the GAE. What did you get right in your answer to Problem 1? What did you get wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run KMeans on the learned embeddings node_embeddings\n",
    "kmeans = KMeans(n_clusters=6, init=\"random\", n_init=10, random_state=1234)\n",
    "\n",
    "# Assign nodes to classes according to which cluster they belong\n",
    "cluster_labels_GAE = kmeans.fit_predict(node_embeddings_GAE.cpu().numpy())  # Assuming z is a torch tensor\n",
    "\n",
    "# Get true labels\n",
    "true_labels = data_onehot.y.cpu().numpy()  # Ground truth labels from PyG Data object\n",
    "\n",
    "# Plot side-by-side comparison\n",
    "_, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Show both cluster and true plots side by side\n",
    "plot_graph(G_sub, cluster_labels_GAE, title = \"KMeans Cluster Labels from GAE Embeddings\", ax=axes[1])\n",
    "plot_graph(G_sub, true_labels, title = \"True Classes\", ax=axes[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2\n",
    "\n",
    "Put your answer to question 2 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Node Features in GAE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a graph convolutional neural network that uses the feature vectors but no other training data. We'll use the same model just a different set of input data. Thus, there is no need to define the encoder and training function again. We'll instantiate the model, train, and look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_epochs: int = 200\n",
    "\n",
    "# Model setup\n",
    "encoder = GCNEncoder(in_channels=data_cora.num_node_features, hidden1=HIDDEN_1_NODES, hidden2=HIDDEN_2_NODES, out_channels=EMBEDDING_DIMENSION)\n",
    "model = GAE(encoder)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss = train(model, data_cora, optimizer)\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:3d} | Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    node_embeddings_GAE_features = model(data_cora.x, data_cora.edge_index)  # Output shape: [250, 64]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize whether the result is likely to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(node_embeddings_GAE_features, \n",
    "             data_cora.y.cpu(),\n",
    "             title=\"t-SNE 2D Projection of GAE Embeddings with Feature Vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Answer this question before running the next cell. \n",
    "\n",
    "Based on patterns you see in the scatterplot above, what will happen when the 32-dimensional embeddings are clustered into 6 clusters (since the seventh class never appears in the data)? In other words, suppose that we find six clusters and then label each node by its cluster. How well will these node labels correspond to the actual node labels? Justify your answer by looking at the ground truth plot we generated above and discussing what kinds of correlations you see between the graph structure and the node class. Remember that this problem uses the node feature set instead of one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 3\n",
    "\n",
    "Put your answer here\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Run the following cell, which shows a side-by-side comparison of the node classes of ground truth versus the labels produced by the GAE. What did you get right in your answer to Problem 3? What did you get wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run KMeans on the learned embeddings node_embeddings\n",
    "kmeans = KMeans(n_clusters=6, init=\"random\", n_init=10, random_state=1234)\n",
    "\n",
    "# Assign nodes to classes according to which cluster they belong\n",
    "cluster_labels_gae_cora = kmeans.fit_predict(node_embeddings_GAE_features.cpu().numpy())  # Assuming z is a torch tensor\n",
    "\n",
    "# Get true labels\n",
    "true_labels = data_cora.y.cpu().numpy()  # Ground truth labels from PyG Data object\n",
    "\n",
    "# Plot side-by-side comparison\n",
    "_, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Show both cluster and true plots side by side\n",
    "plot_graph(G_sub, cluster_labels_gae_cora, title = \"KMeans Cluster Labels from GAE Embeddings on Cora features\", ax=axes[1])\n",
    "plot_graph(G_sub, true_labels, title = \"True Classes\", ax=axes[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 4\n",
    "\n",
    "Put your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding semi-supervised learning to the GCN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the GCN to do learning, but with classifier error. Since we'll be using the same encoder, we don't need to repeat it. However, we will repeat it and give it a different name. We are doing this because the dimension of the output layer won't be the embedding dimension, but rather the number of desired node classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GCN with classifier output\n",
    "class GCNClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden1, hidden2, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden1)\n",
    "        self.conv2 = GCNConv(hidden1, hidden2)\n",
    "        self.conv3 = GCNConv(hidden2, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training will use a different error since the goal is not to find the 32-dimensional embedding but rather to estimate the node's class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big difference is that we are using training data and compute cross entropy loss, which is a measure of the difference between the predicted class and the actual class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now instantiate the class and train it. Recall how the autoencode instantiated a model: \n",
    "\n",
    "`encoder = GCNEncoder(in_channels=data_onehot.num_node_features, hidden1=HIDDEN_1_NODES, hidden2=HIDDEN_2_NODES, out_channels=EMBEDDING_DIMENSION)`\n",
    "sets the number of output channels to the embedding dimension followed by the instantiation \n",
    "\n",
    "`model = GAE(encoder)`\n",
    "\n",
    "which creates the model as a graph autoencoder.\n",
    "\n",
    "The classifier model uses `out_channels=num_classes` and instantiates the entire model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = GCNClassifier(in_channels=data_cora.num_node_features, hidden1=HIDDEN_1_NODES, hidden2=HIDDEN_2_NODES, out_channels=num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train only on labeled nodes\n",
    "for epoch in range(1, 201):\n",
    "    loss, out = train(model, data_cora, optimizer)\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        model.eval()\n",
    "        pred = out.argmax(dim=1)\n",
    "        acc = (pred[data_cora.train_mask] == data_cora.y[data_cora.train_mask]).float().mean().item()\n",
    "        print(f\"Epoch {epoch:3d} | Loss: {loss:.4f} | Acc (overall): {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We output an _accuracy_ score, which is how well the model is learning the training set. The `pred=out.argmax(dim=1)` chooses the class by finding the class with highest output score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize results, comparing what happens when we use the embedding plus some clusters to what happens when we just run the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings (use softmax output or conv2 layer as embedding)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    node_embeddings = model(data_cora)\n",
    "\n",
    "print(\"Embeddings shape:\", node_embeddings.shape)\n",
    "print(\"Label shape:\", data_cora.y.shape)\n",
    "\n",
    "plot_scatter(node_embeddings, \n",
    "             data_cora.y.cpu(),\n",
    "             title=\"t-SNE 2D Projection of Classifier Embedding with Feature Vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Answer this question before running the next cell.\n",
    "\n",
    "The embedding produced above uses information about correct node classes. How well will we be able to find clusters in the embedding and use those to predict the class of each node? And how well will this prediction compare to if we just use the classifier prediction directly? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code and look at the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clustering on embeddings ---\n",
    "kmeans = KMeans(n_clusters=num_classes, random_state=0)\n",
    "cluster_labels_classifier_embedding = kmeans.fit_predict(node_embeddings.cpu().numpy())\n",
    "\n",
    "# --- Classification prediction ---\n",
    "predicted_labels = node_embeddings.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "# --- True labels ---\n",
    "true_labels = data_cora.y.cpu().numpy()\n",
    "\n",
    "_, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "plot_graph(G_sub, true_labels, title=\"True Labels\", ax=axes[0])\n",
    "plot_graph(G_sub, cluster_labels_classifier_embedding, title=\"KMeans Clusters on GCN Embeddings\", ax=axes[1])\n",
    "plot_graph(G_sub, predicted_labels, title=\"GCN Predicted Labels (argmax)\", ax=axes[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "What did you get right and wrong in your answer to problem 5? What did you learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARI: Quantifying Perforance\n",
    "\n",
    "The Adjusted Rand Index (ARI) measures how similar two groupins are, adjusting for random chance. Prompting ChatGPT-4o notes that ARI evaluates\n",
    "\n",
    "- Which pairs of points were grouped together\n",
    "- Which pairs were placed in different groups\n",
    "- Whether this matches what happened in the ground truth labels\n",
    "\n",
    "ChatGPT also suggests the following interpretations for ARI ranges.\n",
    "\n",
    "**Adjusted Rand Index (ARI) Interpretation**\n",
    "\n",
    "| ARI Score | Interpretation                       |\n",
    "|-----------|--------------------------------------|\n",
    "| **1.0**   | Perfect match with true labels       |\n",
    "| 0.8–0.9   | Excellent clustering                  |\n",
    "| 0.5–0.7   | Good structure, moderate alignment    |\n",
    "| 0.2–0.4   | Weak clustering, some structure       |\n",
    "| **0.0**   | No better than random assignment      |\n",
    "| **< 0.0** | Worse than random (actively misleading) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "Answer this question before running the code.\n",
    "\n",
    "Put the following four ways of classifying nodes in order from smallest to largest ARI scores. Justify your answer.\n",
    "- Classes from GAE with one-hot encoding\n",
    "- Classes from GAE with bag-of-words node feature encoding\n",
    "- Classes from GCN classifier with clustered embedding\n",
    "- Classes from GCN classifier output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 7\n",
    "\n",
    "Put your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and print out the ARI for each classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the ARI for the GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari = adjusted_rand_score(data_onehot.y.cpu().numpy(), cluster_labels_GAE)\n",
    "print(f\"Adjusted Rand Index (ARI) comparing GAE clusters with \\none-hot encoding to true labels: {ari:.4f}\")\n",
    "\n",
    "ari = adjusted_rand_score(data_onehot.y.cpu().numpy(), cluster_labels_gae_cora)\n",
    "print(f\"Adjusted Rand Index (ARI) comparing GAE clusters with \\nbag-of-words encoding to true labels: {ari:.4f}\")\n",
    "\n",
    "# ARI: KMeans on GCN embeddings\n",
    "ari_kmeans_classifier = adjusted_rand_score(true_labels, cluster_labels_classifier_embedding)\n",
    "\n",
    "# ARI: Classifier predictions\n",
    "ari_classifier = adjusted_rand_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"Adjusted Rand Index (KMeans on embeddings): {ari_kmeans_classifier:.4f}\")\n",
    "print(f\"Adjusted Rand Index (GCN predicted labels): {ari_classifier:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "What did you get right and what did you get wrong? Why might you have gotten the answer wrong? Try changing the code so that 30% of the data is used for training (search on `training_percent` to find where to set it). What do you learn from that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 8\n",
    "\n",
    "Put your answer here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
