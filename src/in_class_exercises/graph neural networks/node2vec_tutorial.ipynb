{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial on random-walk based node embeddings using node2vec.\n",
    "Code modified version of the code from page 54 of _Hands-On Graph Neutral Networks Using Python_ by Maxime Labonne\n",
    "\n",
    "Mike Goodrich\n",
    "CS 575\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the karate graph to explore what kind of information random walks reveals. Let's plot the karate graph using labels from the nodes to indicate color. The labels come from whether the member of the karate club stayed with the club (\"club\") or went with the breakoff group (\"officer).\n",
    "\n",
    "Let's import the graph and create a list of labels to use when we plot the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize random walks versus biased random walks in the Karate graph\"\"\"\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "G: nx.Graph = nx.karate_club_graph()\n",
    "# The karate club split into two groups as members followed \n",
    "# Mr. Hi (=0) or one of the club officers (=1)\n",
    "labels: list[str] = []\n",
    "for node in G.nodes:\n",
    "    label = G.nodes[node]['club']\n",
    "    labels.append(1 if label == 'Officer' else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that will handle drawing. Since we'll draw a lot, it will be nice to have this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_network(G,colors,title=\"karate network\",show_scale = False, show_degree_as_size = False, show_labels = True):\n",
    "    #plt.figure(figsize=(5,5),dpi=300)\n",
    "    plt.figure()\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    if show_degree_as_size:\n",
    "        my_node_size=[v * 10 for v in dict(G.degree).values()]\n",
    "    else: my_node_size = 200\n",
    "    nx.draw_networkx(G, \n",
    "                 #pos=nx.spring_layout(G,seed=0), \n",
    "                 pos = nx.nx_pydot.graphviz_layout(G, prog = \"neato\"),\n",
    "                 #pos = nx.fruchterman_reingold_layout(G,seed=0),\n",
    "                 node_color=colors,\n",
    "                 node_size=my_node_size,\n",
    "                 cmap='cool',\n",
    "                 font_size=9,\n",
    "                 font_color='white',\n",
    "                 with_labels=show_labels)\n",
    "    if show_scale == True:\n",
    "        sm = plt.cm.ScalarMappable(cmap = 'cool',norm=plt.Normalize(vmin = 0, vmax=max(colors)))\n",
    "        _ = plt.colorbar(sm, ax=plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the karate graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_network(G,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's define an unbiased random walk function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Hashable\n",
    "def unbiased_random_walk(G: nx.Graph,\n",
    "                         start: Hashable,\n",
    "                         length: int\n",
    "                         ) -> list[Hashable]:\n",
    "    #function for performing uniformly random walk\n",
    "    walk: list[Hashable] = [start] # starting node\n",
    "    for _ in range(length):\n",
    "        neighbors = [node for node in G.neighbors(start)]\n",
    "        next_node = np.random.choice(neighbors,1)[0]\n",
    "        walk.append(next_node)\n",
    "        start = next_node\n",
    "    return walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking where deep walks go if the walks begin at node 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pg 39 of book\n",
    "WALK_LENGTH = 10\n",
    "NUM_TRIALS = 200\n",
    "\n",
    "node_count_dict = dict.fromkeys(G.nodes,0)\n",
    "walks = []\n",
    "for node in G.nodes:\n",
    "    for _ in range(NUM_TRIALS):\n",
    "        walk = unbiased_random_walk(G,node,WALK_LENGTH)\n",
    "        walks.append(walk)\n",
    "\n",
    "        # Compute frequency of different nodes on random walks that start at node 0\n",
    "        if node == 0: \n",
    "            for visited_node in walk:\n",
    "                node_count_dict[visited_node] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the set of walks into the probability that a node was visited. Plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(node_count_dict, num_trials):\n",
    "    node_probabilities = []\n",
    "    for node in node_count_dict.keys():\n",
    "        node_probabilities.append(min(1,node_count_dict[node]/num_trials))\n",
    "    return node_probabilities\n",
    "\n",
    "node_probabilities = normalize(node_count_dict, NUM_TRIALS)\n",
    "\n",
    "draw_network(G,node_probabilities,\n",
    "             title=\"Uniform random walk from node 0\", \n",
    "             show_scale=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node2vec's biased random walks\n",
    "\n",
    "In the unbiased random walks, the probability of the next node depended only on the current node. An unbiased random walk is therefore a first order Markov process defined by the transition probability\n",
    "\n",
    "$$ p(v_{t+1} | v_t) $$\n",
    "\n",
    "By contrast, the node2vec algorithm allows us to decide whether the random walk should be biased toward depth-first exploration or should be biased toward breadth-first exploration.\n",
    " - Depth-first exploration tends to spend roughly equal amounts of time at nodes whose _roles_ in the network are similar, e.g., hubs, leafs, etc. Thus, depth-first exploration tends to identify regular equivalences.\n",
    " - Breadth-first exploration tends to spend time in nearby node neighborhoods. Thus, depth-first exploration tends to identify communities as collections of similar nodes, which is a form of  homophily.\n",
    "\n",
    "Node two vec implements this as a second order Markov process defined by the transition probability\n",
    "\n",
    "$$ p(v_{t+1} | v_t, v_{t-1}) $$\n",
    "\n",
    "Let's discuss this second order Markov process including how node2vec specifies parameters that allow us to emphasize depth-first or breadth-first random walks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure is modified from [Figure 2](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) in the node2vec paper.\n",
    "\n",
    "<img src=\"figures/biased_2nd_order_random_walk.png\" width=\"300\" alt=\"Figure showing a biased random walk\">\n",
    "\n",
    "The transitions in this figure show two parameters, $p$ and $q$. Let's talk about what they mean.\n",
    "\n",
    " - The parameter $q$ says how much we want the biased random walk to emphasize depth first search. Small values of $q$, meaning $0 < q < 1$, indicate that the next node in the random walk tends toward $v_{t+1} = v_{\\rm far}$.\n",
    " - The parameter $p$ says how much we want the random walk to emphasize breadth first search. Small values of $p$, meaning $0 < p < 1 $, indicate that the random walk tends toward returning to node $v_t$'s parent, $v_{t+1} = v_{t-1}$.\n",
    " - The transition between $v_t$ and $v_{\\rm nbr}$ also emphasizes a random walk, but it makes sure that the random walk goes to a unique node in the neighborhood rather than back to the parent node. The figure shows that $v_{\\rm nbr}$ has to be adjacent to node $v_t$ (solid arrow) and adjacent to the parent node $v_{t-1}$ (dashed line).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the edges do not add up to one, which is a problem if we want to implement the random walk using actual transition probabilities. We fix this by normalizing using the following pseudocode:\n",
    " - for each $v \\in V$ where $V$ is the set of vertices\n",
    "   - if $v = v_{t-1}$: $p(v) = 1/p $\n",
    "   - else if $v\\in v {\\rm neighborhood}$: $p(v) = 1$\n",
    "   - else if $v \\in v_{\\rm far}$: $p_(v) = 1/q$\n",
    "   - else $p(v) = 0$.\n",
    "The order of the if-else statements are important. If a vertex is both a parent and a neighbor then its probability should be $1/p$. If a vertex is a neighbor but not a parent then its probability should be $1$. If a vertex is not a neighbor nor a parent but is adjacent then its probability should be $1/q$. Otherwise, its probability is $0$.\n",
    "\n",
    "\n",
    "We then normalize to get the transition probability\n",
    "\n",
    "$$ p(v | v_{t}, v_{t-1}) = \\frac{p(v)}{\\sum_{u\\in V} p(v)} $$\n",
    "\n",
    "We implement this in three functions. See the function description for information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Biased random walks ...\"\"\"\n",
    "# pg 55 of book\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "def biased_next_node(G,previous,current,p,q):\n",
    "    ''' Get the probabilities using the parameters and returns the next node '''\n",
    "    neighbors = list(G.neighbors(current))\n",
    "    alphas = []\n",
    "    for neighbor in neighbors:\n",
    "        if neighbor == previous:\n",
    "            alpha = 1/p\n",
    "        elif G.has_edge(neighbor,previous):\n",
    "            alpha = 1\n",
    "        else:\n",
    "            alpha = 1/q\n",
    "        alphas.append(alpha)\n",
    "    probs = [alpha/sum(alphas) for alpha in alphas]\n",
    "    next = np.random.choice(neighbors, p=probs)\n",
    "    return next\n",
    "\n",
    "def biased_random_walk(G,start,length,p,q):\n",
    "    ''' Implement a biased random walk by choosing nodes from the biased_next_node function'''\n",
    "    #function for performing biased random walk\n",
    "    walk = [start] # starting node\n",
    "    for _ in range(length):\n",
    "        current = walk[-1]\n",
    "        previous = walk[-2] if len(walk) > 1 else None\n",
    "        next = biased_next_node(G, previous, current, p, q)\n",
    "        walk.append(next)\n",
    "    return walk\n",
    "\n",
    "def get_node_probabilities(G,start_node,walk_length,num_trials,p,q):\n",
    "    ''' Compute how often each node is visited on a biased random walk from start_node'''\n",
    "    node_count_dict = dict.fromkeys(sorted(G.nodes),0)\n",
    "    node_probabilities = []\n",
    "    walks = []\n",
    "    for _ in range(num_trials):\n",
    "        walk = biased_random_walk(G,start_node,walk_length,p,q)\n",
    "        walks.append(walk)\n",
    "        # Compute frequency of different nodes on random walks that start at start_node\n",
    "        for visited_node in walk:\n",
    "            node_count_dict[visited_node] += 1\n",
    "    normalizer = sum(node_count_dict.keys())\n",
    "    for node in node_count_dict.keys():\n",
    "        node_probabilities.append(node_count_dict[node]/normalizer)\n",
    "    return node_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what happens if we bias the walk toward depth-first search. We previously said that depth-first searches tend to discover regular equivalence, so let's see if that's correct. A depth first search has a small $q$ and a large $p$. I'm going to choose long walks to really emphasize the point. I'll also show the size of the node proportional to its degree since degree is a first-order approximation of regular equivalence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WALK_LENGTH = 30\n",
    "NUM_TRIALS = 200\n",
    "p = 10\n",
    "q = 0.001 # Depth first search\n",
    "start_node = 0\n",
    "\n",
    "# Color a node by it's random walk distance from node zero\n",
    "node_probabilities = get_node_probabilities(G,start_node,WALK_LENGTH,NUM_TRIALS,p,q)\n",
    "graph_title = f\"Biased DFS random walk: p = {p}, q = {q}\"\n",
    "draw_network(G,node_probabilities, title=graph_title, show_scale = True, show_degree_as_size = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the hub node 33 is the node whose values are most close to node 0, which is also a hub node. This is a form of regular equivalence. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's bias the random walk so that it emphasizes breadth first search. This should pull out homophily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WALK_LENGTH = 30\n",
    "NUM_TRIALS = 200\n",
    "p = 10\n",
    "q = 1000 # Emphasize breadth first search\n",
    "start_node = 0\n",
    "\n",
    "# Color a node by it's random walk distance from node zero\n",
    "node_probabilities = get_node_probabilities(G,start_node,WALK_LENGTH,NUM_TRIALS,p,q)\n",
    "graph_title = f\"Biased BFS random walk: p = {p}, q = {q}\"\n",
    "draw_network(G,node_probabilities, title=graph_title, show_scale = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the nodes most similar to node 0 are those nodes that are in node 0's neighborhood.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering by homophily\n",
    "\n",
    "Let's put the pieces together and consider random walks biased towards DFS and random walks biased towards BFS. We'll then do the same kind of clustering that we did in the deepwalk tutorial. I'm going to copy the biased random walk function here so that we can see it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect walks starting from each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WALK_LENGTH = 30\n",
    "NUM_TRIALS = 200\n",
    "p = 10; q = 1000 # emphasizes homophily (neighbors via breadth first search)\n",
    "walks = []\n",
    "for node in G.nodes:\n",
    "    for _ in range(NUM_TRIALS):\n",
    "        walks.append(biased_random_walk(G,node,WALK_LENGTH,p,q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use skip gram to get embedding from the set of walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "def get_trained_model(walks, walk_length, embedding_dimension):\n",
    "    model = Word2Vec(walks,\n",
    "                    hs=1, #softmax = 0, hierarchical softmax = 1\n",
    "                    sg=1, #skip-gram\n",
    "                    vector_size=embedding_dimension,\n",
    "                    window=walk_length,\n",
    "                    workers=2, negative = 10,\n",
    "                    alpha = 0.03,\n",
    "                    seed=0)\n",
    "    model.train(walks,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=30, \n",
    "                report_delay=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 64\n",
    "model = get_trained_model(walks, WALK_LENGTH ,embedding_dimension)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which nodes are closest to node 1 in the embedding? Note that default distance in Word2Vec is _cosine similarity_, ${\\mathbf x}_i^T {\\mathbf x}_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_node = 0\n",
    "closest: list[tuple] = model.wv.most_similar(target_node)\n",
    "count = 0\n",
    "for t in closest:\n",
    "    print(t)\n",
    "    count+= 1\n",
    "    if count == 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are a lot of dimensions to the embedding returned by the skipgram model (e.g., word2vec), let's look at the first two embedding dimensions. Other embedding dimensions might show something different so treat this as a very rough check on whether clustering might be occuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_embedding(G, embedding):\n",
    "    for i, node in enumerate(G.nodes):\n",
    "        label = G.nodes[node]['club']\n",
    "        color = ('c' if label == 'Officer' else 'm')\n",
    "        plt.scatter(embedding[i,0], embedding[i,1],s=100,alpha = 0.8, color = color)\n",
    "        plt.annotate(node, xy=(embedding[i,0], embedding[i,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's actually get the embedding and the show it. Then we can do a visual inspection to see if we think things will cluster well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Use TSNE to compress from 100 dimensional embedding to two\n",
    "X = model.wv[G.nodes]\n",
    "embedding = TSNE(n_components=2).fit_transform(X)\n",
    "show_embedding(G, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there will be good clustering here. Let's cluster the nodes into a handful groups. W We can then compare the clusters we find to how the karate graph actually split. Hopefully, we'll learn a bit about how well structural/relational similarity matched what happend in the karate class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up kmeans clustering\n",
    "from sklearn.cluster import KMeans\n",
    "def get_clusters(embedding, num_clusters = 4):\n",
    "    kmeans = KMeans(\n",
    "        init=\"random\",\n",
    "        n_clusters=num_clusters,\n",
    "        n_init=10,\n",
    "        random_state=1234\n",
    "        )\n",
    "    kmeans.fit(embedding)\n",
    "    return kmeans\n",
    "\n",
    "kmeans = get_clusters(embedding)\n",
    "print(kmeans.labels_[:10])\n",
    "\n",
    "# See how things clustered\n",
    "labels = kmeans.labels_\n",
    "draw_network(G, labels, title = \"Nodes colored by homophily-biased walk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Clustering by regular equivalence\n",
    "\n",
    "Let's repeat but with more balance between the depth-first and breadth-first searches in node2vec. That should pickup up similarity that includes both structural and relational components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WALK_LENGTH = 30\n",
    "NUM_TRIALS = 200\n",
    "p = 0.1; q = 0.001 # emphasizes structure via DFS\n",
    "walks = []\n",
    "for node in G.nodes:\n",
    "    for _ in range(NUM_TRIALS):\n",
    "        walks.append(biased_random_walk(G,node,WALK_LENGTH,p,q))\n",
    "\n",
    "embedding_dimension = 64\n",
    "model = get_trained_model(walks, WALK_LENGTH ,embedding_dimension)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "# Use TSNE to compress from 100 dimensional embedding to two\n",
    "X = model.wv[G.nodes]\n",
    "embedding = TSNE(n_components=2).fit_transform(X)\n",
    "show_embedding(G, embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster and plot by cluster. I'm going to use fewer clusters because that illustrates how depth first search emphasizes regular equivalence. I'll also show node size proportional to degree to emphasize regular equivalence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = get_clusters(embedding,2)\n",
    "# See how things clustered\n",
    "draw_network(G, kmeans.labels_, title = \"Nodes colored by DFS-biased walk\", show_degree_as_size=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is still strong clustering by local community, but that node 0 and node 33 are clustered together. Note further that I had to really tweak parameters to make this work. Most of the parameter sets had node 0 and node 33 in different clusters.\n",
    "\n",
    "Try it for a different number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = get_clusters(embedding,3)\n",
    "# See how things clustered\n",
    "draw_network(G, kmeans.labels_, title = \"Nodes colored by DFS-biased walk\", show_degree_as_size=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to the Les Miserables network\n",
    "\n",
    "Let's see if we can use node2vec to recreate Figure 3 in the [original node2vec paper](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.les_miserables_graph()\n",
    "draw_network(G, colors = 'y', show_degree_as_size=True, show_labels=False, title = \"Les Miserables graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WALK_LENGTH = 30\n",
    "NUM_TRIALS = 200\n",
    "p = 0.1\n",
    "q = 0.001 # emphasizes structure via DFS\n",
    "num_clusters = 4\n",
    "walks = []\n",
    "for node in G.nodes:\n",
    "    for _ in range(NUM_TRIALS):\n",
    "        walks.append(biased_random_walk(G,node,WALK_LENGTH,p,q))\n",
    "\n",
    "embedding_dimension = 64\n",
    "model = get_trained_model(walks, WALK_LENGTH ,embedding_dimension)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "# Use TSNE to compress from 100 dimensional embedding to two\n",
    "X = model.wv[G.nodes]\n",
    "embedding = TSNE(n_components=2).fit_transform(X)\n",
    "# cluster\n",
    "kmeans = get_clusters(embedding,num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorlist = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00', '#ffff33', '#a65628']\n",
    "pos = dict()\n",
    "labels = []\n",
    "for i, node in enumerate(G.nodes):\n",
    "    pos[node] = embedding[i,:]\n",
    "    color = colorlist[kmeans.labels_[i]]\n",
    "    labels.append(color)\n",
    "    plt.scatter(embedding[i,0], embedding[i,1],s=100,alpha = 0.8, color = color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how things clustered\n",
    "draw_network(G, kmeans.labels_, title = \"Nodes colored by DFS-biased walk\", show_degree_as_size=True, show_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Now emphasizing structural equivalence/ homophily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "WALK_LENGTH = 30\n",
    "NUM_TRIALS = 200\n",
    "p = 10\n",
    "q = 100 # emphasizes homophily\n",
    "num_clusters = 6\n",
    "walks = []\n",
    "for node in G.nodes:\n",
    "    for _ in range(NUM_TRIALS):\n",
    "        walks.append(biased_random_walk(G,node,WALK_LENGTH,p,q))\n",
    "\n",
    "embedding_dimension = 64\n",
    "model = get_trained_model(walks, WALK_LENGTH ,embedding_dimension)\n",
    "\n",
    "\n",
    "# Use TSNE to compress from 100 dimensional embedding to two\n",
    "X = model.wv[G.nodes]\n",
    "embedding = TSNE(n_components=2).fit_transform(X)\n",
    "# cluster\n",
    "kmeans = get_clusters(embedding,num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorlist = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00', '#ffff33', '#a65628']\n",
    "#pos = dict()\n",
    "labels = []\n",
    "for i, node in enumerate(G.nodes):\n",
    "    #pos[node] = embedding[i,:]\n",
    "    color = colorlist[kmeans.labels_[i]]\n",
    "    labels.append(color)\n",
    "    plt.scatter(embedding[i,0], embedding[i,1],s=100,alpha = 0.8, color = color)\n",
    "    #plt.annotate(node, xy=(embedding[i,0], embedding[i,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how things clustered\n",
    "draw_network(G, kmeans.labels_, title = \"Nodes colored by BFS-biased walk\", show_degree_as_size=True, show_labels=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
