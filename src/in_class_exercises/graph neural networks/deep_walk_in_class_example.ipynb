{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deepwalk\n",
    "\n",
    "This tutorial discusses the Deepwalk algorithm. The algorithm is a method for finding node embeddings. \n",
    "\n",
    "The Deepwalk algorithm was presented in \n",
    "```\n",
    "        Perozzi, B., Al-Rfou, R., & Skiena, S. (2014, August). Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 701-710).\n",
    "```\n",
    "\n",
    "The code for the Deepwalk portion of this Jupyter notebook tutorial is a modified version of the code from page 40 of _Hands-On Graph Neutral Networks Using Python_ by Maxime Labonne.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node Embedding\n",
    "\n",
    "Recall the definition of a node embedding: \"A node embedding is a low-dimensional vector that has the property that two nodes that are similar to each other in the graph are close to each the vector space.\" (I've lost the source of this quote so please don't use it.)\n",
    "\n",
    "We want to find a definition of node similarity that represences abstract ideas of _structural equivalence_ or _relational equivalence_ and represents the similarity as a low dimensional vector\n",
    "\n",
    "The Deepwalk algorithm says that two nodes are similar if they are visited about the same number of times as each other on a random walk. The frequency with which a node is visited defines the embedding, and such embeddings are called _random-walk node embedding_. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Walks on the Karate Club Network**\n",
    "\n",
    "We'll explore the basics of the Deepwalk algorithm on the Karate network. The club split into two groups, one led by an _Officer_ in the original club and the other led by a club member known as _Mr. Hi_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from typing import Hashable\n",
    "from drawing_utilities import show_partitions\n",
    "from drawing_utilities import show_partitions_with_scaled_nodesize\n",
    "from drawing_utilities import show_node_probability\n",
    "from drawing_utilities import show_graph\n",
    "\n",
    "G: nx.Graph = nx.karate_club_graph()\n",
    "partition: dict[str,list[Hashable]] = {'Mr. Hi': [], 'Officer': []}\n",
    "for node in G.nodes():\n",
    "    partition[G.nodes[node]['club']].append(node)\n",
    "show_partitions(G, list(partition.values()), title = \"Karate club network\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show nodes with node size scaled to degree. Nodes with high degree are likely to be visited more on random walks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_partitions_with_scaled_nodesize(G, list(partition.values()), title = \"Karate club network\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unbiased Random Walks**\n",
    "\n",
    "We'll define three functions:\n",
    "- A function that captures an unbiased random walk. \n",
    "- A function that takes a starting node, a walk length, and a number of trials and returns a dictionary of {node: number of visits}\n",
    "- A function that assigns a probability that a node was visited on a particular random walk given the following inputs \n",
    "  - dictionary of {node: number of visits} and\n",
    "  - the number of trials \n",
    "\n",
    "Let's define those functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)\n",
    "def unbiased_random_walk(G: nx.Graph,\n",
    "                         start: Hashable,\n",
    "                         length: int\n",
    "                         ) -> list[Hashable]:\n",
    "    #function for performing uniformly random walk\n",
    "    walk: list[Hashable] = [start] # starting node\n",
    "    current_node: Hashable = start\n",
    "    for _ in range(length):\n",
    "        neighbors = [node for node in G.neighbors(current_node)]\n",
    "        next_node = random.choice(neighbors)\n",
    "        walk.append(next_node)\n",
    "        current_node = next_node\n",
    "    return walk\n",
    "\n",
    "def count_node_visits(G: nx.Graph, \n",
    "                      starting_node: Hashable, \n",
    "                      walk_length: int, \n",
    "                      num_trials: int\n",
    "                      ) -> dict[Hashable, int]:\n",
    "    node_count_dict = dict.fromkeys(G.nodes,0)\n",
    "    walks = []\n",
    "    for _ in range(num_trials):\n",
    "        walk = unbiased_random_walk(G,starting_node,walk_length)\n",
    "        walks.append(walk)\n",
    "    for walk in walks:\n",
    "        for visited_node in walk:\n",
    "            node_count_dict[visited_node] += 1\n",
    "    return node_count_dict\n",
    "\n",
    "def normalize(node_count_dict: dict[Hashable, int],\n",
    "              num_trials: int\n",
    "              ) -> list[float]:\n",
    "    node_probabilities: list[float] = []\n",
    "    for node in node_count_dict.keys():\n",
    "        node_probabilities.append(min(1,node_count_dict[node]/num_trials))\n",
    "    return node_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequency of Node Visits**\n",
    "\n",
    "Build some intuition by collecting a bunch of random walks that start at a particular node. Then plot the frequency with which various other nodes in the network were visited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## CELL 1 ##\n",
    "############\n",
    "WALK_LENGTH = 10\n",
    "NUM_TRIALS = 200\n",
    "STARTING_NODE = 0\n",
    "\n",
    "node_count_dict: dict = count_node_visits(G, STARTING_NODE, WALK_LENGTH, NUM_TRIALS)\n",
    "node_probabilities = normalize(node_count_dict, NUM_TRIALS)\n",
    "\n",
    "show_node_probability(G,\n",
    "                      node_probabilities,\n",
    "                      title=f\"Uniform random walk with length {WALK_LENGTH} from node {STARTING_NODE}\", \n",
    "                      show_scale=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the following exercises:\n",
    " - shorten the path length and discuss what happens\n",
    " - increase the path length and discuss what happens\n",
    " - start from a node on periphery of the graph and discuss what happens\n",
    " - start with a high degree node and discuss what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## Cell 2 ##\n",
    "############\n",
    "\n",
    "WALK_LENGTH = 5\n",
    "NUM_TRIALS = 200\n",
    "STARTING_NODE = 0\n",
    "\n",
    "node_count_dict: dict = count_node_visits(G, STARTING_NODE, WALK_LENGTH, NUM_TRIALS)\n",
    "node_probabilities = normalize(node_count_dict, NUM_TRIALS)\n",
    "\n",
    "show_node_probability(G,\n",
    "                      node_probabilities,\n",
    "                      title=f\"Uniform random walk with length {WALK_LENGTH} from node {STARTING_NODE}\", \n",
    "                      show_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## Cell 3 ##\n",
    "############\n",
    "\n",
    "WALK_LENGTH = 10\n",
    "NUM_TRIALS = 200\n",
    "STARTING_NODE = 15\n",
    "\n",
    "node_count_dict: dict = count_node_visits(G, STARTING_NODE, WALK_LENGTH, NUM_TRIALS)\n",
    "node_probabilities = normalize(node_count_dict, NUM_TRIALS)\n",
    "\n",
    "show_node_probability(G,\n",
    "                      node_probabilities,\n",
    "                      title=f\"Uniform random walk with length {WALK_LENGTH} from node {STARTING_NODE}\", \n",
    "                      show_scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deepwalk Core Idea\n",
    "\n",
    "The basic idea of the Deepwalk algrithm is to take a bunch of random walks and then find a way to use how often nodes appear on the same walk to represent node similarity. Deepwalk doesn't just count the number of times a node is visted, and instead uses an anology that involves words.  The analogy is this: _If you take a very large corpus of literatue and then start to look at how often words appear close to each other, you get a sense of how \"similar\" the words are to each other_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Frequency to Co-Occurrence**\n",
    "\n",
    "The random-walk analog for \"two words are similar if they occur close to each other in common sentences\" is \"to nodes are similar if they occur close to each other on a lot of random walks.\"\n",
    "\n",
    "Define a method that collects some random walks from a starting node of a particular length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_walks(G: nx.Graph, \n",
    "              starting_node: Hashable, \n",
    "              walk_length: int, \n",
    "              num_walks: Hashable\n",
    "              ) -> list[Hashable]:\n",
    "    walks: list[Hashable] = []\n",
    "    for _ in range(num_walks):\n",
    "        walk = unbiased_random_walk(G,starting_node,walk_length)\n",
    "        walks.append([str(node) for node in walk])\n",
    "    return walks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's peak at what a handful of these walks look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## CELL 4 ##\n",
    "############\n",
    "\n",
    "STARTING_NODE = 5\n",
    "WALK_LENGTH = 3\n",
    "NUM_WALKS = 5\n",
    "walks = get_walks(G, STARTING_NODE, WALK_LENGTH, NUM_WALKS)\n",
    "for walk in walks: \n",
    "    print(walk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collecting Random Walks into Training Data**\n",
    "\n",
    "We do a set of these \"deep walks\" starting at each node in the graph, and collect all the walks into a \"corpus\" of walks. Folloiwng the example in chapter 3 of _Hands-On Graph Neural Networks Using Python_, Maxime Labonne.Let's do 100 random walks for each starting node, and limit the number of steps in each walk to 10. I'll print out the first two random walks that start at node 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## CELL 5 ##\n",
    "############\n",
    "\n",
    "WALK_LENGTH = 10\n",
    "NUM_WALKS = 100\n",
    "\n",
    "walks: list[Hashable] = []\n",
    "\n",
    "for node in G.nodes():\n",
    "    walks.extend(get_walks(G, node, WALK_LENGTH, NUM_WALKS))\n",
    "\n",
    "# Look at some walks that begin at node 0 and at node 1    \n",
    "node: int = 0\n",
    "print(f\"The first walk that begins at node {node} = \\n  {walks[NUM_WALKS*node + 0]}\")\n",
    "print(f\"The second walk that begins at node {node} = \\n  {walks[node + 1]}\")\n",
    "node: int = 1\n",
    "print(f\"The first walk that begins at node {node} = \\n  {walks[NUM_WALKS*node + 0]}\")\n",
    "print(f\"The second walk that begins at node {node} = \\n  {walks[NUM_WALKS*node + 1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "Now that we have training data, we will train a model. See the accompanying slide deck. Also see [Word2Vec in Python](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html)\n",
    "\n",
    "We don't have a lot of data (100 walks for each node), so don't try to make emedding dimenions that are too big. According to the things I've read and seen, 100 dimensions is pretty typical. That seems a little strange since there are only 34 nodes in the karate graph, but let's see where it leads us.  I'm going to set the seed so that each time I run the simulation I get the same results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Code**\n",
    "\n",
    "Deepwalk uses an existing training model based on the well-known _Skipgram_ model for embedding words. Implementing the code from Maximme's book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## CELL 6 ##\n",
    "############\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "embedding_dimension = 100 \n",
    "## parameters from the book\n",
    "model = Word2Vec(walks,\n",
    "                 hs=1, #softmax = 0, hierarchical softmax = 1\n",
    "                 sg=1, #skip-gram\n",
    "                 vector_size=embedding_dimension,\n",
    "                 window=WALK_LENGTH,\n",
    "                 workers=2, negative = 10,\n",
    "                 seed=0)\n",
    "model.train(walks,\n",
    "            total_examples=model.corpus_count,\n",
    "            epochs=30, \n",
    "            report_delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at which nodes are similar to which other nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_node = 0\n",
    "string_node = str(target_node)\n",
    "print(f'Nodes that are most similar to node {target_node}:')\n",
    "for similarity in model.wv.most_similar(positive=[string_node]):\n",
    "    print(f' {similarity}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes 0 and 33 play similar roles in the karate club network, so let's check out how similar they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Similarity between node 0 and node 33: {model.wv.similarity('0','33')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's low. Why? Because deep random walks like this emphasize structural equivalence rather than relational equivalence. Let's look at one of the nodes in the general neighborhood of node 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Similarity between node 0 and node 16: {model.wv.similarity('0','16')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much higher. Why? Because node 0 and node 16 share a lot common neighbors. That suggests that we can probably cluster nodes into common neighborhoods.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens when we apply this to the pinwheel graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## CELL 7 ##\n",
    "############\n",
    "\n",
    "pinwheel_graph = nx.Graph()\n",
    "pinwheel_graph.add_nodes_from([0,1,2,3,4,5,6,7,8,9])\n",
    "pinwheel_graph.add_edges_from([(1,2),(1,3),(2,3),(4,5),(4,6),(5,6),(7,8),(7,9),(8,9),(1,0),(4,0),(7,0)])\n",
    "show_graph(pinwheel_graph, title=\"Pinwheel network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## CELL 8 ##\n",
    "############\n",
    "\n",
    "WALK_LENGTH = 8\n",
    "NUM_WALKS = 80\n",
    "\n",
    "walks: list[Hashable] = []\n",
    "\n",
    "for node in pinwheel_graph.nodes():\n",
    "    walks.extend(get_walks(pinwheel_graph, node, WALK_LENGTH, NUM_WALKS))\n",
    "\n",
    "print(\"Walks that start at node 0:\")\n",
    "print(\"\\t\",walks[0])\n",
    "print(\"\\t\",walks[1])\n",
    "print(\"Walks that start at node 1:\")\n",
    "print(\"\\t\",walks[80])\n",
    "print(\"\\t\",walks[81])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model. Note that this is a small graph, so we don't want the embedding dimension to be too big. If we try to make the embedding dimension too big, there won't be enough data to learn the matrix parameters, and we'll end up with garbage. Some key parameters are\n",
    " - hs \n",
    "    - when = 0 means we use the softmax like discussed in the slides\n",
    "    - when = 1 means we use the hierarchical softmax method defined in the original Deepwalk paper to speed up learning\n",
    " - sg = 1 means that we use the skipgram model like we defined it in the slides\n",
    " - vector_size = the size of the matrix we are trying to learn in the hidden layer. This specifies how many dimensions our embedding ${\\mathbb R}^d$ has.\n",
    " - window. Don't confuse the length of the walk used to create the set of random walks with the length of the window used to create the co-occurrence account. In this example, the length of the window equals the length of the walk\n",
    " - the other parameters require more deep understanding for how the neural network training occurs, so we won't discuss this in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## CELL 9 ##\n",
    "############\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "embedding_dimension = 8\n",
    "print(f\"Window length = {WALK_LENGTH}\")\n",
    "print(f\"Embedding dimension = {embedding_dimension}\")\n",
    "model = Word2Vec(walks,\n",
    "                 hs=0, #softmax = 0, hierarchical softmax = 1\n",
    "                 sg=1, #skip-gram\n",
    "                 vector_size=embedding_dimension,\n",
    "                 window=WALK_LENGTH,\n",
    "                 workers=2, negative = 10,\n",
    "                 seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "## CELL 10 ##\n",
    "#############\n",
    "\n",
    "model.train(walks,\n",
    "            total_examples=model.corpus_count,\n",
    "            epochs=30, \n",
    "            report_delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the nodes most similar to node 4 to help us build intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_node = 4\n",
    "string_node = str(target_node)\n",
    "print(f'Nodes that are most similar to node {target_node}:')\n",
    "for similarity in model.wv.most_similar(positive=[string_node]):\n",
    "    print(f' {similarity}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the matrix that was learned. The raw values probably don't mean a lot to you, but notice that the dimensions of the matrix. It has 10 columns, one for each node in the pinwheel network, and 8 rows, one for each of the dimensions we specified in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_wv = np.array([model.wv.get_vector(str(i)) for i in range(len(model.wv))])\n",
    "print(np.transpose(np.round(nodes_wv,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node has it's own vector because we were computing\n",
    "\n",
    "$$ f : V \\rightarrow {\\mathbb R}^d $$\n",
    "\n",
    "We chose $d=10$. Let's inspect the row for a few of the nodes. All we are doing here is pulling out the column associated with particular nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_node = 2\n",
    "print(\"The embedding for node\",target_node)\n",
    "print(f\"  {np.round(model.wv.get_vector(str(target_node)),2)}\")\n",
    "target_node = 5\n",
    "print(\"The embedding for node\",target_node)\n",
    "print(f\"  {np.round(model.wv.get_vector(str(target_node)),2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a well-known algorithm for taking the $8\\times 10$ embedding matrix and turning it into 2D vector. Essentially, we want to compress all the information from the embedding matrix into the best two dimensions so that we can visualize the results. The algorithm we'll use is called TSNE. You can use it without understanding it well -- that's what I'm doing this semester.\n",
    "\n",
    "We did something similar with the spectrum of the Laplacian matrix of the adjacency matrix, but we did this by hand by just choosing our favorite two eigenvectors. TSNE says \"You might now know which parts of the matrix are most important, so I'll help you find them.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "## CELL 11 ##\n",
    "#############\n",
    "from sklearn.manifold import TSNE \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define parameters\n",
    "embedding_dimension = 64\n",
    "print(f\"Window length = {WALK_LENGTH}\")\n",
    "print(f\"Embedding dimension = {embedding_dimension}\")\n",
    "\n",
    "# Define model\n",
    "model = Word2Vec(walks,\n",
    "                 hs=0, #softmax = 0, hierarchical softmax = 1\n",
    "                 sg=1, #skip-gram\n",
    "                 vector_size=embedding_dimension,\n",
    "                 window=WALK_LENGTH,\n",
    "                 workers=2, negative = 10,\n",
    "                 seed=0)\n",
    "\n",
    "# Train model\n",
    "model.train(walks,\n",
    "            total_examples=model.corpus_count,\n",
    "            epochs=30, \n",
    "            report_delay=1)\n",
    "\n",
    "# Extract embedding\n",
    "nodes_wv = np.array([model.wv.get_vector(str(i)) for i in range(len(model.wv))])\n",
    "\n",
    "# Compress embedding to 2 dimensions\n",
    "num_dimensions = 2\n",
    "tsne = TSNE(n_components=num_dimensions,\n",
    "            learning_rate='auto',\n",
    "            init='pca',\n",
    "            random_state=0,\n",
    "            perplexity=5.0).fit_transform(nodes_wv)\n",
    "\n",
    "# Plot compressed embedding\n",
    "_ = plt.scatter(tsne[:, 0], tsne[:, 1], s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cluster like we did with the spectrum of $L$ and the spectrum of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "## CELL 12 ##\n",
    "#############\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters=4,\n",
    "    n_init=10,\n",
    "    random_state=1234\n",
    "    )\n",
    "kmeans.fit(tsne)\n",
    "\n",
    "# See how things clustered\n",
    "labels = kmeans.labels_\n",
    "partition: dict[int, list[Hashable]] = {key:[] for key in sorted(labels)}\n",
    "for node in pinwheel_graph:\n",
    "    partition[labels[node]].append(node)\n",
    "show_partitions(pinwheel_graph, partition=list(partition.values()), title=\"Deepwalk classification of pinwheel graph:\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
